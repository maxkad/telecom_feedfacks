import logging
import re
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone, ServerlessSpec
from typing import List, Dict, Any


class PineconeSemanticSearch:
    """
    A modular class for managing semantic embeddings in Pinecone.
    Supports inserting new documents and querying similar texts.
    """

    # -------------------------------------------------------
    # Initialization
    # -------------------------------------------------------
    def __init__(
                    self, 
                    pinecone_index_name,
                    pinecone_api_key,
                    embed_model_name
                ):

        # Environment variables
        self.pinecone_api_key = pinecone_api_key
        self.embed_model_name = embed_model_name
        self.pinecone_index_name = pinecone_index_name

        # Initialize Pinecone client (new API)
        self.pc = Pinecone(api_key=self.pinecone_api_key)
        self.index = self.pc.Index(self.pinecone_index_name)

        # Load model once
        self.model = SentenceTransformer(self.embed_model_name)
        logging.info(f"PineconeSemanticSearch initialized for index: {self.pinecone_index_name}")


    # -------------------------------------------------------
    # Static Method: Normalize text
    # -------------------------------------------------------
    @staticmethod
    def normalize_text(text: str) -> str:
        """Normalize user text before embedding."""
        return str(text).strip().lower()


    # -------------------------------------------------------
    # Query Pinecone
    # -------------------------------------------------------
    def query_pinecone(self, user_query: str, filter_condition: int = None, top_k: int = 5) -> List[Dict[str, Any]]:
        """Perform semantic search on Pinecone."""
        if not user_query or not user_query.strip():
            logging.warning("Empty query.")
            return []

        logging.info(f"Querying: '{user_query}' (top_k={top_k})")

        query_vector = self.model.encode(self.normalize_text(user_query)).tolist()

        try:

            if filter_condition is not None and filter_condition >= 1 and filter_condition <= 5:

                if ("×žÖ¾" in user_query or "×ž×¢×œ" in user_query
                        or "×’×‘×•×” ×ž" in user_query
                        or "×ž×¢×œ ×œ" in user_query):

                    # Example: filter by 'level' metadata field
                    response = self.index.query(
                        vector=query_vector,
                        top_k=top_k,
                        include_metadata=True,
                        filter={"level": {"$gt": filter_condition}}
                        )
                elif "×©×•×•×™× ×œ" in user_query or "×©×•×•×” ×œ" in user_query:
                    # Example: filter by 'level' metadata field
                    response = self.index.query(
                        vector=query_vector,
                        top_k=top_k,
                        include_metadata=True,
                        filter={"level": {"$eq": filter_condition}}
                        )

                elif "×‘×™×Ÿ" in user_query:
                    # Example: filter by 'level' metadata field
                    response = self.index.query(
                        vector=query_vector,
                        top_k=top_k,
                        include_metadata=True,
                        filter={"level": {"$gte": filter_condition - 1, "$lte": filter_condition + 1}}  # ðŸ‘ˆ Filter condition
                        )

                elif ("×¢×“" in user_query or "×¢×“ ×œ" in user_query 
                        or "× ×ž×•×š ×ž" in user_query 
                        or "×¤×—×•×ª ×ž" in user_query 
                        or "×ž×ª×—×ª" in user_query or "×ž×ª×—×ª ×œ" in user_query):
                # Example: filter by 'level' metadata field
                    response = self.index.query(
                        vector=query_vector,
                        top_k=top_k,
                        include_metadata=True,
                        filter={"level": {"$eq": filter_condition}}
                        )
                
            elif (
                "× ×ž×•×š" in user_query 
                or "×¨×ž×” × ×ž×•×›×”" in user_query
                or "×¦×™×•×Ÿ × ×ž×•×š" in user_query
                or "× ×ž×•×›×”" in user_query
                or "×œ× ×ž×¨×•×¦×™×" in user_query
                or "×œ× ×ž×¨×•×¦×”" in user_query
                or "×‘×¢×™×•×ª ×©×™×¨×•×ª" in user_query
                or "×©×™×¨×•×ª ×œ×§×•×™" in user_query
                or "×©×™×¨×•×ª ×’×¨×•×¢" in user_query
                or "×œ× ×˜×•×‘" in user_query
                or "×œ× ×˜×•×‘×”" in user_query
                or "×ª×œ×•× ×•×ª" in user_query
                or "×ª×œ×•× ×”" in user_query):

                response = self.index.query(
                    vector=query_vector,
                    top_k=top_k,
                    include_metadata=True,
                    filter={"level": {"$lt": 3}}
                    )
            elif (
                "×’×‘×•×”" in user_query
                or "×¨×ž×” ×’×‘×•×”×”" in user_query
                or "×ž×¨×•×¦×™×" in user_query
                or "×ž×¨×•×¦×”" in user_query
                or "×©×™×¨×•×ª ×˜×•×‘" in user_query
                or "×©×™×¨×•×ª ×ž×¦×•×™×Ÿ" in user_query
                or "×©×™×¨×•×ª ×ž×¢×•×œ×”" in user_query
                or "×©×™×¨×•×ª × ×”×“×¨" in user_query
                or "×©×™×¨×•×ª ×ž×¦×˜×™×™×Ÿ" in user_query
                or "×—×•×•×™×™×ª ×©×™×¨×•×ª ×˜×•×‘×”" in user_query
                or "×—×•×•×™×ª ×©×™×¨×•×ª ×˜×•×‘×”" in user_query
                or "×ž×¦×•×™×Ÿ" in user_query
                or "×ž×¢×•×œ×”" in user_query
                or "× ×”×“×¨" in user_query):

                response = self.index.query(
                    vector=query_vector,
                    top_k=top_k,
                    include_metadata=True,
                    filter={"level": {"$gt": 3}}  
                    )
            else:
                response = self.index.query(vector=query_vector, top_k=top_k, include_metadata=True)

            matches = response.get("matches", [])
            logging.info(f"Found {len(matches)} matches.")
            return matches
        
        except Exception as e:
            logging.error(f"Query failed: {e}")
            return []


    def extract_number(self, user_query: str):
        """
        Extract the first number (digit) appearing in the query.
        Works with Hebrew queries like '×žÖ¾3', '×ž×¢×œ 4', etc.
        Returns int or None.
        """
        
        # Normalize various Hebrew dash characters and spacing
        user_query = user_query.replace("Ö¾", "-").replace("â€“", "-").replace("×ž-", " ").replace("×žÖ¾", " ")

        # Regular expression: find one or more digits
        match = re.search(r"\d+", user_query)
        if match:
            return int(match.group(0))
        return None


    # -------------------------------------------------------
    # Print Results Helper
    # -------------------------------------------------------
    @staticmethod
    def print_results(matches: List[Dict[str, Any]]):
        """Pretty print Pinecone search results."""
        if not matches:
            print("No matches found.")
            return

        print("Top Semantic Matches:")
        for match in matches:
            meta = match.get("metadata", {})
            print(f"â€¢ ID: {match.get('id')}")
            print(f"  Score: {match.get('score', 0):.4f}")
            print(f"  Text: {meta.get('text', '')[:120]}...\n")



    @staticmethod
    def get_context(matches: List[Dict[str, Any]]):
        """Pretty print Pinecone search results."""
        if not matches:
            print("No matches found.")
            return
        else:
            context_texts = []
            for match in matches:
                meta = match.get("metadata", {})
                text = meta.get("text") or meta.get("chunk_text") or "No text available"
                context_texts.append(f"â€¢ {text}")
            context = "\n".join(context_texts)
        return context
